import pandas as pd
import numpy as np
import scipy.stats as stats
import itertools as itert

## constants

RESULT_DF_HEADER_old = ['attr1','attr2','allCorr','subgroupCorr','groupbyAttr','subgroup']

RESULT_DF_HEADER = ['feat1','feat2','Trend_type','agg_Trend','group_feat',
                    'subgroup','subgroup_Trend']




from .Trends import all_Trend_types


def get_views(df,colored=False):
    """
    return a list of tuples of the views of the dataset that have at least one
    Trend. Assumes no views are listed in in opposite orders

    Parameters
    -----------
    result_df : DataFrame
        reustls generated by detect_simpsons_paradox
    colored : Boolean
        use 'colored' views or not if True, a view is defined by 2 features and
        a grouping variable, if False a view is defined by 2 features only


    Returns
    ---------
    sp_views_unique : list of tuples
        list of the view pairs

    """
    # get all the views in a zip iterator
    if colored:
        views_per_occurence = zip(df.feat1.values,
                                    df.feat2.values,
                                    df.group_feat.values)
        views_unique = set([(a,b,c) for a,b,c in views_per_occurence])
    else:
        # uncolored (no grouping var)
        views_per_occurence = zip(df.feat1.values,
                                 df.feat2.values)
        # iterate over pairs to make list, get unique by type casting to a set
        views_unique = set([(a,b) for a,b in views_per_occurence])

    # type cast back to list to return
    return list(views_unique)

################################################################################
# helper Mixin class
################################################################################
class _TrendDetectors():
    """
    a mixin class of detectors and Trend computations
    """


    def get_SP_views(self,thresh=0, colored=False):
        """
        return a list of tuples of the views of the dataset that have at least one
        occurence of SP. Assumes no views are listed in in opposite orders

        Parameters
        -----------
        result_df : DataFrame
            reustls generated by get_subgroup_Trends_*
        sp_type : string or function handle
            SP type as defined by label_SP_rows()
        cols_pair : list of strings
            column anmes to be used in the SP detection
        colored : Boolean
            use 'colored' views or not if True, a view is defined by 2 features and
            a grouping variable, if False a view is defined by 2 features only

        Returns
        ---------
        sp_views_unique : list of tuples
            list of the view pairs

        """

        # filter
        sp_df = self.get_SP_rows(thresh)


        return get_views(sp_df,colored)

    def get_SP_rows(self,thresh=None,inplace=False,replace=False):
        """
        return a list of tuples of the rows of the dataset that have at least one
        occurence of SP.

        Parameters
        -----------
        result_df : DataFrame
            reustls generated by get_subgroup_Trends_*
        sp_type : string or function handle
            SP type as defined by label_SP_rows()
        cols_pair : list of strings
            column anmes to be used in the SP detection
        colored : Boolean
            use 'colored' views or not if True, a view is defined by 2 features and
            a grouping variable, if False a view is defined by 2 features only

        Returns
        ---------
        sp_views_unique : list of tuples
            list of the view pairs

        """
        # col_name
        if thresh:

            if type(thresh) == dict:
                col_name = thresh['name']
            elif type(thresh) == str:
                col_name = thresh
            else:
                # for compatibility with old passing a distance threshold
                col_name = 'SP_thresh' +str(thresh)
                thresh = {'distance':thresh,'name':col_name}

        else:
            thresh = 'SP'
            col_name = 'SP'

        # label if not
        if not(col_name in self.result_df.columns ) or replace:
            # add dist if not
            if not('distance' in self.result_df.columns):
                self.add_distance()

            col_name = self.label_SP_rows(thresh)



        # always filter result and return
        sp_df = self.result_df[self.result_df[col_name]]

        # overwrite if inplace
        if inplace:
            self.result_df = sp_df

        return sp_df



    def get_subgroup_Trends_1lev(self,Trend_types, replace=False):
        """
        find subgroup and aggregate Trends in the dataset, return a DataFrame that
        contains information necessary to filter for SP and relaxations
        computes for 1 level grouby (eg correlation and linear Trends)

        Parameters
        -----------
        labeled_df : LabeledDataFrame
            data to find SP in, must be tidy
        Trend_types: list of strings or list Trend objects
            info on what Trends to compute and the variables to use, dict is of form
        {'name':<str>,'vars':['varname1','varname1'],'func':functionhandle}

        """
        data_df = self.df
        groupby_vars = self.get_vars_per_role('groupby')


        if type(Trend_types[0]) is str:
            # instantiate objects
            self.Trend_list.extend([all_Trend_types[Trend]()
                                                    for Trend in Trend_types])
        else:
            # use provided, must be instantiated
            self.Trend_list.extend(Trend_types)

        # prep the result df to add data to later
        self.result_df = pd.DataFrame(columns=RESULT_DF_HEADER)

        # create empty lists
        all_Trends = []
        subgroup_Trends = []

        for cur_Trend in self.Trend_list:
            cur_Trend.get_Trend_vars(self)

            # Tabulate aggregate statistics
            agg_Trends = cur_Trend.get_Trends(self.df,'agg_Trend')

            all_Trends.append(agg_Trends)

            # iterate over groupby attributes
            for groupbyAttr in groupby_vars:

                #condition the data
                cur_grouping = self.df.groupby(groupbyAttr)

                # get subgoup Trends
                curgroup_corr = cur_Trend.get_Trends(cur_grouping,'subgroup_Trend')

                # append
                subgroup_Trends.append(curgroup_corr)




        # condense and merge all Trends with subgroup Trends
        all_Trends = pd.concat(all_Trends)
        subgroup_Trends = pd.concat(subgroup_Trends)
        if self.result_df.empty or replace:
            self.result_df = pd.merge(subgroup_Trends,all_Trends)
        else:
            new_res = pd.merge(subgroup_Trends,all_Trends)
            self.result_df = pd.concat([self.result_df,new_res])
        # ,on=['feat1','feat2'], how='left

        # remove rows where a Trend is undefined
        self.result_df.dropna(subset=['subgroup_Trend','agg_Trend'],axis=0,inplace=True)

        return self.result_df

    def get_subgroup_Trends_2lev(self,Trend_types):
        """
        find subgroup and aggregate Trends in the dataset,

        Parameters
        -----------

        """

        data_df = self.df
        groupby_vars = self.get_vars_per_role('groupby')


        if type(Trend_types[0]) is str:
            # instantiate objects
            self.Trend_list = [all_Trend_types[Trend]() for Trend in Trend_types]
        else:
            # use provided
            self.Trend_list = Trend_types

        # prep the result df to add data to later
        self.result_df = pd.DataFrame(columns=RESULT_DF_HEADER)

        # create empty lists
        all_Trends = []
        subgroup_Trends = []

        for td in Trend_dict_list:
            Trend_func = td['func']
            Trend_vars = td['vars']
            # Tabulate aggregate statistics
            agg_Trends = Trend_func(data_df,Trend_vars,'agg_Trend')

            all_Trends.append(agg_Trends)

            # iterate over groupby attributes
            for groupbyAttr in groupby_vars:
                # add groupbyAttr to list of splits
                Trend_vars_gb = [tv.append(groupbyAttr) for tv in Trend_vars]

                # get subgoup Trends
                curgroup_corr = Trend_func(cur_grouping,Trend_vars_gb,'subgroup_Trend')

                # append
                subgroup_Trends.append(curgroup_corr)




        # condense and merge all Trends with subgroup Trends
        all_Trends = pd.concat(all_Trends)
        subgroup_Trends = pd.concat(subgroup_Trends)
        result_df = pd.merge(subgroup_Trends,all_Trends)
        # ,on=['feat1','feat2'], how='left

        return result_df









################################################################################
# helper functions
################################################################################


# Function s
def upper_triangle_element(matrix):
    """
    extract upper triangle elements without diagonal element

    Parameters
    -----------
    matrix : 2d numpy array

    Returns
    --------
    elements : numpy array
               A array has all the values in the upper half of the input matrix

    """
    #upper triangle construction
    tri_upper = np.triu(matrix, k=1)
    num_rows = tri_upper.shape[0]

    #upper triangle element extract
    elements = tri_upper[np.triu_indices(num_rows,k=1)]

    return elements


def upper_triangle_df(matrix):
    """
    extract upper triangle elements without diagonal element and store the element's
    corresponding rows and columns' index information into a dataframe

    Parameters
    -----------
    matrix : 2d numpy array

    Returns
    --------
    result_df : dataframe
        A dataframe stores all the values in the upper half of the input matrix and
    their corresponding rows and columns' index information into a dataframe
    """
    #upper triangle construction
    tri_upper = np.triu(matrix, k=1)
    num_rows = tri_upper.shape[0]

    #upper triangle element extract
    elements = tri_upper[np.triu_indices(num_rows,k=1)]
    location_tuple = np.triu_indices(num_rows,k=1)
    result_df = pd.DataFrame({'value':elements})
    result_df['attr1'] = location_tuple[0]
    result_df['attr2'] = location_tuple[1]

    return result_df

def isReverse(a, b):
    """
    Reversal is the logical opposite of signs matching.

    Parameters
    -----------
    a : number(int or float)
    b : number(int or float)

    Returns
    --------
    boolean value : If True turns, a and b have the reverse sign.
                    If False returns, a and b have the same sign.
    """

    return not (np.sign(a) == np.sign(b))



def detect_simpsons_paradox(data_df,
                            regression_vars=None,
                            groupby_vars=None,type='linreg' ):
    """
    LEGACY
    A detection function which can detect Simpson Paradox happened in the data's
    subgroup.

    Parameters
    -----------
    data_df : DataFrame
        data organized in a pandas dataframe containing both categorical
        and continuous attributes.
    regression_vars : list [None]
        list of continuous attributes by name in dataframe, if None will be
        detected by all float64 type columns in dataframe
    groupby_vars  : list [None]
        list of group by attributes by name in dataframe, if None will be
        detected by all object and int64 type columns in dataframe
    type : {'linreg',} ['linreg']
        default is linreg for backward compatibility


    Returns
    --------
    result_df : dataframe
        a dataframe with columns ['attr1','attr2',...]
                TODO: Clarify the return information

    """
    # if not specified, detect continous attributes and categorical attributes
    # from dataset
    if groupby_vars is None:
        groupbyAttrs = data_df.select_dtypes(include=['object','int64'])
        groupby_vars = list(groupbyAttrs)

    if regression_vars is None:
        continuousAttrs = data_df.select_dtypes(include=['float64'])
        regression_vars = list(continuousAttrs)


    # Compute correaltion matrix for all of the data, then extract the upper
    # triangle of the matrix.
    # Generate the correaltion dataframe by correlation values.
    all_corr = data_df[regression_vars].corr()
    all_corr_df = upper_triangle_df(all_corr)
    all_corr_element = all_corr_df['value'].values

    # Define an empty dataframe for result
    result_df = pd.DataFrame(columns=RESULT_DF_HEADER)

    # Loop by group-by attributes
    for groupbyAttr in groupby_vars:
        grouped_df_corr = data_df.groupby(groupbyAttr)[regression_vars].corr()
        groupby_value = grouped_df_corr.index.get_level_values(groupbyAttr).unique()

        # Get subgroup correlation
        for subgroup in groupby_value:
            subgroup_corr = grouped_df_corr.loc[subgroup]

            # Extract subgroup
            subgroup_corr_elements = upper_triangle_element(subgroup_corr)

            # Compare the signs of each element in subgroup to the correlation for all of the data
            # Get the index for reverse element
            index_list = [i for i, (a,b) in enumerate(zip(all_corr_element, subgroup_corr_elements)) if isReverse(a, b)]

            # Get reverse elements' correlation values
            reverse_list = [j for i, j in zip(all_corr_element, subgroup_corr_elements) if isReverse(i, j)]

            if reverse_list:
                # Retrieve attribute information from all_corr_df
                all_corr_info = [all_corr_df.loc[i].values for i in index_list]
                temp_df = pd.DataFrame(data=all_corr_info,columns=['agg_Trend','feat1','feat2'])

                # # Convert index from float to int
                temp_df.feat1 = temp_df.feat1.astype(int)
                temp_df.feat2 = temp_df.feat2.astype(int)
                # Convert indices to attribute names for readabiity
                temp_df.feat1 = temp_df.feat1.replace({i:a for i, a in
                                            enumerate(regression_vars)})
                temp_df.feat2 = temp_df.feat2.replace({i:a for i, a in
                                            enumerate(regression_vars)})

                temp_df['subgroup_Trend'] = reverse_list
                len_list = len(reverse_list)
                # Store group attributes' information
                temp_df['group_feat'] = [groupbyAttr for i in range(len_list)]
                temp_df['subgroup'] = [subgroup for i in range(len_list)]
                result_df = result_df.append(temp_df, ignore_index=True)

    return result_df




# def detect_sp_pandas():
#     total_data = np.asarray([np.sign(many_sp_df_diff.corr().values)]*3).reshape(18,6)
#     A_data = np.sign(many_sp_df_diff.groupby('A').corr().values)
#     sp_mask = total_data*A_data
# sp_idx = np.argwhere(sp_mask<0)
# #     Comput corr, take sign
# # conditionn and compute suc corrs, take sign
# # mutliply two together, all negative arer SP
# # get locations to dtermine laels
#     labels_levels = many_sp_df_diff.groupby('A').corr().index
#     groupByAttr_list = [labels_levels.levels[0][ll] for ll in labels_levels.labels[0]]
#     var_list_dn  = [labels_levels.levels[1][li] for li in labels_levels.labels[1]]
#     var_list_ac = many_sp_df_diff.groupby('A').corr().columns
#     # labels_levels.levels
#     SP_cases = [(groupByAttr_list[r],var_list_dn[r],var_list_ac[c]) for r,c in sp_idx ]
